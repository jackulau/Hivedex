{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hivedex API Testing Notebook\n",
    "\n",
    "Test all data sources and signal calculations before deploying to Hex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install requests pandas numpy vaderSentiment yfinance tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Arctic Shift API (Reddit Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Arctic Shift API\n",
    "url = 'https://arctic-shift.photon-reddit.com/api/posts/search'\n",
    "params = {\n",
    "    'subreddit': 'wallstreetbets',\n",
    "    'title': 'NVDA',\n",
    "    'limit': 10\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params, timeout=30)\n",
    "print(f'Status: {response.status_code}')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    posts = data.get('data', [])\n",
    "    print(f'Posts returned: {len(posts)}')\n",
    "    \n",
    "    if posts:\n",
    "        df = pd.DataFrame(posts)\n",
    "        display(df[['title', 'score', 'num_comments', 'created_utc']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test GDELT API (News Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GDELT DOC 2.0 API\n",
    "url = 'https://api.gdeltproject.org/api/v2/doc/doc'\n",
    "params = {\n",
    "    'query': 'NVIDIA earnings',\n",
    "    'mode': 'artlist',\n",
    "    'maxrecords': 10,\n",
    "    'format': 'json',\n",
    "    'startdatetime': '20241101000000',\n",
    "    'enddatetime': '20241120235959'\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params, timeout=60)\n",
    "print(f'Status: {response.status_code}')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    articles = data.get('articles', [])\n",
    "    print(f'Articles returned: {len(articles)}')\n",
    "    \n",
    "    if articles:\n",
    "        df = pd.DataFrame(articles)\n",
    "        display(df[['title', 'domain', 'seendate']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test yfinance (Stock Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "stock = yf.Ticker('NVDA')\n",
    "history = stock.history(start='2024-11-01', end='2024-11-25')\n",
    "\n",
    "print(f'Days of data: {len(history)}')\n",
    "display(history.head())\n",
    "\n",
    "# Calculate price change\n",
    "if not history.empty:\n",
    "    change = ((history['Close'].iloc[-1] / history['Close'].iloc[0]) - 1) * 100\n",
    "    print(f'Price change: {change:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test VADER Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "test_texts = [\n",
    "    'NVDA to the moon! Best earnings ever!',\n",
    "    'This stock is going to crash hard',\n",
    "    'Just bought some shares',\n",
    "    'Terrible investment, stay away',\n",
    "    'Nvidia beats expectations, AI demand strong'\n",
    "]\n",
    "\n",
    "results = []\n",
    "for text in test_texts:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'compound': scores['compound'],\n",
    "        'pos': scores['pos'],\n",
    "        'neg': scores['neg']\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Data Fetcher Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_fetcher import fetch_reddit_posts, fetch_gdelt_news\n",
    "\n",
    "# Fetch Reddit posts\n",
    "posts = fetch_reddit_posts(\n",
    "    subreddits=['wallstreetbets'],\n",
    "    keywords=['NVDA', 'nvidia'],\n",
    "    start_date='2024-11-01',\n",
    "    end_date='2024-11-20',\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "print(f'Fetched {len(posts)} Reddit posts')\n",
    "if not posts.empty:\n",
    "    display(posts[['title', 'score', 'created_utc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch GDELT news\n",
    "news = fetch_gdelt_news(\n",
    "    keywords=['NVIDIA', 'earnings'],\n",
    "    start_date='2024-11-01',\n",
    "    end_date='2024-11-20',\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "print(f'Fetched {len(news)} news articles')\n",
    "if not news.empty:\n",
    "    display(news[['title', 'domain', 'seendate']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Signal Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from signal_calculator import (\n",
    "    add_sentiment_to_posts,\n",
    "    calculate_reddit_signal,\n",
    "    calculate_gdelt_signal,\n",
    "    calculate_combined_signal,\n",
    "    calculate_lead_time\n",
    ")\n",
    "\n",
    "# Add sentiment to posts\n",
    "if not posts.empty:\n",
    "    posts_with_sentiment = add_sentiment_to_posts(posts)\n",
    "    print(f'Average sentiment: {posts_with_sentiment[\"compound\"].mean():.3f}')\n",
    "    \n",
    "    # Calculate Reddit signal\n",
    "    reddit_signal = calculate_reddit_signal(posts_with_sentiment)\n",
    "    print(f'\\nReddit Signal Summary:')\n",
    "    print(f'  Days analyzed: {len(reddit_signal)}')\n",
    "    print(f'  Average signal: {reddit_signal[\"reddit_signal\"].mean():.2f}')\n",
    "    print(f'  Peak signal: {reddit_signal[\"reddit_signal\"].max():.2f}')\n",
    "    \n",
    "    display(reddit_signal[['date', 'volume', 'sentiment', 'reddit_signal']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate GDELT signal\n",
    "if not news.empty:\n",
    "    gdelt_signal = calculate_gdelt_signal(news)\n",
    "    print(f'GDELT Signal Summary:')\n",
    "    print(f'  Days analyzed: {len(gdelt_signal)}')\n",
    "    print(f'  Average signal: {gdelt_signal[\"gdelt_signal\"].mean():.2f}')\n",
    "    \n",
    "    display(gdelt_signal[['date', 'coverage', 'gdelt_signal']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine signals\n",
    "if not posts.empty and not news.empty:\n",
    "    combined = calculate_combined_signal(reddit_signal, gdelt_signal)\n",
    "    \n",
    "    print(f'Combined Signal Summary:')\n",
    "    print(f'  Days analyzed: {len(combined)}')\n",
    "    print(f'  Average hivemind signal: {combined[\"hivemind_signal\"].mean():.2f}')\n",
    "    \n",
    "    # Calculate lead time\n",
    "    lead_time = calculate_lead_time(combined, '2024-11-20')\n",
    "    print(f'\\nLead Time:')\n",
    "    for k, v in lead_time.items():\n",
    "        print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "if 'combined' in dir() and not combined.empty:\n",
    "    # Melt for multi-line chart\n",
    "    plot_data = combined[['date', 'reddit_signal', 'gdelt_signal', 'hivemind_signal']].melt(\n",
    "        id_vars=['date'],\n",
    "        var_name='signal_type',\n",
    "        value_name='signal_value'\n",
    "    )\n",
    "    \n",
    "    chart = alt.Chart(plot_data).mark_line(point=True).encode(\n",
    "        x=alt.X('date:T', title='Date'),\n",
    "        y=alt.Y('signal_value:Q', title='Signal (0-100)', scale=alt.Scale(domain=[0, 100])),\n",
    "        color='signal_type:N',\n",
    "        tooltip=['date:T', 'signal_type:N', 'signal_value:Q']\n",
    "    ).properties(\n",
    "        width=700,\n",
    "        height=400,\n",
    "        title='Hivedex Signal Timeline'\n",
    "    ).interactive()\n",
    "    \n",
    "    chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load and View Events Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv('../data/events_catalog.csv', comment='#')\n",
    "print(f'Total events: {len(events)}')\n",
    "print(f'\\nBy category:')\n",
    "print(events['category'].value_counts())\n",
    "\n",
    "display(events.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
